import numpyimport mathfrom cvxopt import matrix, solversfrom urllib.request import urlopendef shuffle(x, y):    tmp = list(zip(x, y))    numpy.random.shuffle(tmp)    x_new = []    y_new = []    for i in range(len(tmp)):        x_new.append(tmp[i][0])        y_new.append(tmp[i][1])    return x_new, y_newdef get_data():    x, y = [], []    file = urlopen("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data")    for line in file.readlines():        input = line.decode('utf-8').strip().split(',')        if input[1] == 'M':            y.append(1.0)        else:            y.append(-1.0)        parameters = [float(x) for x in input[2:]]        parameters.insert(0, 1.0)        parameters = numpy.array(parameters)        x.append(parameters)    file.close()    return shuffle(x, y)def inner_product_kernel(x1, x2):    return numpy.inner(x1, x2)def polynomial_kernel(x1, x2, q = 5):    return (1 + numpy.inner(x1, x2)) ** qdef gaussian_kernel(x1, x2, gamma = 1.0):    diff = numpy.array(x1) - numpy.array(x2)    norm_sq = numpy.inner(diff, diff)    return math.exp(-gamma * norm_sq)def transpose(ls):    return list(map(list, zip(*ls)))def lagr_koef(x, y, const, kernel):    solvers.options['show_progress'] = False    P = numpy.identity(len(x))    for i in range(len(x)):        for j in range(len(x)):            P[i][j] = y[i] * y[j] * kernel(x[i], x[j])    P = matrix(P)    A = matrix(transpose([y]))    b = matrix([0.0])    q = matrix([-1.0] * len(x))    G = numpy.zeros(shape=(len(x) * 2, len(x)))    h = matrix([0.0] * len(x) + [const] * len(x))    for i in range(len(x)):            G[i][i] = -1    for i in range(len(x)):            G[len(x) + i][i] = 1    G = matrix(transpose(G))    sol = solvers.qp(P, q, G, h, A, b)['x']    lagrange = [sol[i] for i in range(len(x))]    return lagrangedef get_w(x, y, C):    lagrange = lagr_koef(x, y, C, inner_product_kernel)    eps = 1e-5    w = numpy.zeros(len(x[0]))    for i in range(len(x)):        if lagrange[i] > eps:            w += lagrange[i] * y[i] * numpy.array(x[i])    b = 0    for i in range(len(x)):        if eps < lagrange[i] < C - eps:            b = y[i] - numpy.inner(w, x[i])            break    return w, bdef kernel_predict(x_out, x_in, y_in, lagrange, kernel):    eps = 1e-5    b = 0    for i in range(len(x_in)):        if lagrange[i] > eps:            b += y_in[i]            for j in range(len(x_in)):                if lagrange[j] > eps:                    b -= lagrange[j] * y_in[j] * kernel(x_in[i], x_in[j])            break    y_out = []    for x_out_cur in x_out:        y_cur = 0        for i in range(len(x_in)):            if lagrange[i] > eps:                y_cur += lagrange[i] * y_in[i] * kernel(x_in[i], x_out_cur)        y_cur += b        y_out.append(1.0 if y_cur > 0 else -1.0)    return y_outdef predict(x, w, b):    return [1.0 if numpy.inner(w, x[i]) + b > 0 else -1.0 for i in range(len(x))]def get_best_constant(x, y, parts):    part_size = int(len(x) / parts)    minEout = 1    best_const = 1    for deg in range(-5, 20):        const = (2.0) ** (deg)        print(deg)        Eout = 0        for i in range(parts):            check_set_x = x[i * part_size: (i + 1) * part_size]            check_set_y = y[i * part_size: (i + 1) * part_size]            training_set_x = x[:i * part_size] + x[(i + 1) * part_size:]            training_set_y = y[:i * part_size] + y[(i + 1) * part_size:]                        lagrange = lagr_koef(training_set_x, training_set_y, const, inner_product_kernel)            prediction = kernel_predict(check_set_x, training_set_x, training_set_y, lagrange, inner_product_kernel)            misclassified = 0            for j in range(len(check_set_x)):                if check_set_y[j] != prediction[j]:                    misclassified += 1            Eout += misclassified / len(check_set_x) / parts        if Eout < minEout:            minEout = Eout            best_const = const    return best_constdef main():    x, y = get_data()    test_size = int(len(x) * 0.2)    check_set_x = x[:test_size]    check_set_y = y[:test_size]    training_set_x = x[test_size:]    training_set_y = y[test_size:]    const = get_best_constant(training_set_x, training_set_y, 10)    lagrange = lagr_koef(training_set_x, training_set_y, const, inner_product_kernel)    prediction = kernel_predict(training_set_x, training_set_x, training_set_y, lagrange, inner_product_kernel)    misclassified = 0    for j in range(len(training_set_x)):            if training_set_y[j] != prediction[j]:                misclassified += 1    Ein = misclassified / len(training_set_x)    prediction = kernel_predict(check_set_x, training_set_x, training_set_y, lagrange, inner_product_kernel)    stats = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}    for j in range(len(check_set_x)):            if check_set_y[j] == -1:                key = 'tn' if prediction[j] == -1 else 'fp'                stats[key] += 1            if check_set_y[j] == 1:                key = 'tp' if prediction[j] == 1 else 'fn'                stats[key] += 1    precision = stats['tp'] / (stats['tp'] + stats['fp'])    recall = stats['tp'] / (stats['tp'] + stats['fn'])    F1 = 2 * precision * recall / (precision + recall)    Eout = (stats['fp'] + stats['fn']) / len(x[:test_size])    print("regularization constant = %6.5f" % const)    print('in sample error = %6.2f' % (100 * Ein))    print('out of sample error = %6.2f' % (100 * Eout))    print('precision = %6.2f' % (100 * precision))    print('recall = %6.2f' % (100 * recall))    print('F1 = %6.2f' % (100 * F1))if __name__ == "__main__":    main()